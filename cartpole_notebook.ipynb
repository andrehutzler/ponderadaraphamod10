{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50c149a8",
   "metadata": {},
   "source": [
    "# CartPole RL com Q-Learning\n",
    "**Aluno:** André \n",
    "**Data:** 20/05/2025  \n",
    "**Descrição:** Implementação de Q-Learning para resolver CartPole-v1 sem redes neurais.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a157e83",
   "metadata": {},
   "source": [
    "## 1. Modelagem como MDP\n",
    "- **Estados (4):** posição, velocidade do carrinho; ângulo, velocidade angular do pêndulo  \n",
    "- **Ações (2):** 0 = esquerda, 1 = direita  \n",
    "- **Recompensa:** +1 a cada passo não terminal  \n",
    "- **Bellman (Q-Learning):**  \n",
    "  $$Q(s,a)\\leftarrow Q(s,a)+\\alpha\\bigl[r + \\gamma\\,\\max_{a'}Q(s',a') - Q(s,a)\\bigr]$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbfe3553",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4b4c3d",
   "metadata": {},
   "source": [
    "## 2. Discretização de Estados\n",
    "Usamos 4 dimensões contínuas mapeadas em `n_bins = (6, 12, 6, 12)`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8f41be",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Parâmetros\n",
    "n_bins = (6, 12, 6, 12)\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "# Limites razoáveis para cada dimensão\n",
    "obs_low  = np.array([-2.4, -3.0, -0.2094, -3.5])\n",
    "obs_high = np.array([ 2.4,  3.0,  0.2094,  3.5])\n",
    "\n",
    "# Cria os pontos de corte (bins)\n",
    "bins = [np.linspace(obs_low[i], obs_high[i], n_bins[i]-1)\n",
    "        for i in range(len(n_bins))]\n",
    "\n",
    "def discretize(obs):\n",
    "    \"\"\"Retorna tupla de índices discretos para um obs contínuo.\"\"\"\n",
    "    state = []\n",
    "    for i, val in enumerate(obs):\n",
    "        idx = np.digitize(val, bins[i])\n",
    "        state.append(idx)\n",
    "    return tuple(state)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526fc629",
   "metadata": {},
   "source": [
    "## 3. Inicialização da Q-Table e Hiperparâmetros\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0914cf8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Q-table com shape = (*n_bins, n_actions)\n",
    "Q_table = np.zeros(n_bins + (env.action_space.n,))\n",
    "\n",
    "# Hiperparâmetros\n",
    "alpha       = 0.1      # learning rate\n",
    "gamma       = 0.99     # discount factor\n",
    "epsilon     = 1.0      # exploração inicial\n",
    "eps_decay   = 0.995\n",
    "min_epsilon = 0.01\n",
    "n_episodes  = 10000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5880c077",
   "metadata": {},
   "source": [
    "## 4. Loop de Treinamento (Q-Learning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5caeba45",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "rewards = []\n",
    "\n",
    "for ep in range(n_episodes):\n",
    "    obs = env.reset()\n",
    "    state = discretize(obs)\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        # política ε-greedy\n",
    "        if np.random.rand() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = np.argmax(Q_table[state])\n",
    "\n",
    "        obs2, reward, done, _ = env.step(action)\n",
    "        state2 = discretize(obs2)\n",
    "\n",
    "        # Q-Learning update\n",
    "        best_next = np.max(Q_table[state2])\n",
    "        Q_table[state + (action,)] += alpha * (\n",
    "            reward + gamma * best_next - Q_table[state + (action,)]\n",
    "        )\n",
    "\n",
    "        state = state2\n",
    "        total_reward += reward\n",
    "\n",
    "    # decaimento de epsilon\n",
    "    epsilon = max(min_epsilon, epsilon * eps_decay)\n",
    "    rewards.append(total_reward)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f15488",
   "metadata": {},
   "source": [
    "## 5. Resultados e Gráficos\n",
    "Média móvel de recompensa (janelas de 100 episódios)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331496dd",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Cálculo da média móvel\n",
    "mov_avg = np.convolve(rewards, np.ones(100)/100, mode='valid')\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(mov_avg)\n",
    "plt.title('Recompensa média (janela=100 eps)')\n",
    "plt.xlabel('Episódios')\n",
    "plt.ylabel('Recompensa média')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
